{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd9842d",
   "metadata": {},
   "source": [
    "# How to compute and load DF (document frequency) counts using `pke`\n",
    "\n",
    "DF counts are required by several keyphrase extraction models (e.g. TfIdf, Kea) for weighting candidates. Below is an example on how to compute DF counts for a given text collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90172a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/boudinfl/pke.git\r\n",
      "  Cloning https://github.com/boudinfl/pke.git to /private/var/folders/_s/dsym612j14gggkqchsd35clh0000gn/T/pip-req-build-2_zafc0t\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /private/var/folders/_s/dsym612j14gggkqchsd35clh0000gn/T/pip-req-build-2_zafc0t\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!pip install datasets\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2fbf76",
   "metadata": {},
   "source": [
    "## Preamble on keyphrase extraction datasets using ðŸ¤— datasets\n",
    "\n",
    "For simplicity and ease of use, we rely on the `datasets` module from ðŸ¤— huggingface to load and access sample documents from keyphrase extraction datasets. Please have a look at our organization page (https://huggingface.co/taln-ls2n) for more information on which datasets are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f2083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load the inspec dataset\n",
    "dataset = load_dataset('taln-ls2n/inspec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "from spacy.tokenizer import _get_regex_pattern\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenization fix for in-word hyphens (e.g. 'non-linear' would be kept \n",
    "# as one token instead of default spacy behavior of 'non', '-', 'linear')\n",
    "# https://spacy.io/usage/linguistic-features#native-tokenizer-additions\n",
    "\n",
    "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "# Modify tokenizer infix patterns\n",
    "infixes = (\n",
    "    LIST_ELLIPSES\n",
    "    + LIST_ICONS\n",
    "    + [\n",
    "        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
    "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "        ),\n",
    "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "        # âœ… Commented out regex that splits on hyphens between letters:\n",
    "        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
    "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
    "    ]\n",
    ")\n",
    "\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp.tokenizer.infix_finditer = infix_re.finditer\n",
    "\n",
    "# populates a docs list with spacy doc objects\n",
    "docs = []\n",
    "for sample in tqdm(dataset['train']):\n",
    "    docs.append(nlp(sample[\"title\"]+\". \"+sample[\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e159c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pke import compute_document_frequency\n",
    "from string import punctuation\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# compute idf weights\n",
    "compute_document_frequency(\n",
    "    documents=docs,\n",
    "    output_file='inspec.df.gz',\n",
    "    language='en',              # language of the input files\n",
    "    normalization='stemming',   # use porter stemmer\n",
    "    stoplist=list(punctuation), # stoplist (punctuation marks)\n",
    "    n=5                         # compute n-grams up to 5-grams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299f0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pke import load_document_frequency_file\n",
    "\n",
    "df = load_document_frequency_file(input_file='inspec.df.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
